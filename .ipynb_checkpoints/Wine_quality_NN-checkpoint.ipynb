{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from sklearn import metrics as m\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"winequality-red.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a mapping dictionary that maps the quality values from 0 to 5\n",
    "quality_mapping = {\n",
    "3: 0, 4: 1, 5: 2, 6: 3, 7: 4, 8: 5}\n",
    "df[\"quality\"] = df[\"quality\"].map(quality_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"acidity_by_Ph\"] = df[\"fixed acidity\"]/df[\"pH\"]\n",
    "df[\"density_sugar\"] = df[\"density\"]/df[\"residual sugar\"]\n",
    "df[\"sulphate_by_chloride\"] = df[\"sulphates\"]/df[\"chlorides\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "corr_vals = abs(corr_matrix[\"quality\"]).sort_values(ascending=False)\n",
    "top8_corr =  corr_vals[1:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                 0.480738\n",
       "volatile acidity        0.391735\n",
       "sulphate_by_chloride    0.363317\n",
       "sulphates               0.270777\n",
       "citric acid             0.233733\n",
       "total sulfur dioxide    0.185404\n",
       "density                 0.173251\n",
       "fixed acidity           0.127766\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top8_corr # we will carry with these attributes for the modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "top8_corr = pd.DataFrame(top8_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the atributes that correlate best with the wine quality are alcohol, volatile acicity, sulphates, citric acid, total sufur dioxide, density, fixed acidity and chlorides. We will be using these in the modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To prevent the model from overfitting I decided to remove two attributes which seemed to not have a major importance and correlation with the target (citric acid and fixed acidity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top8_corr.drop([\"citric acid\", \"fixed acidity\"], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_top8 = list(top8_corr.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_top8.append(\"quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df[list_top8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the selected attributes shown an skewed distribution and possess different scales. We will therefore standarized the data before modeling. But first, lets split the data into the test and train sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Splitting data: \n",
    "First we check the distribution of the target values to see if an stratified split is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a skewed dataset for classification \n",
    "so we may want to stratify the data before split. Furthermore, we will want to use **stratified k-fold cross-validation** (For classification problems).\n",
    "\n",
    "-> There are several choices for selecting the appropriate number of bins. If\n",
    "you have a lot of samples( > 10k, > 100k), then you don’t need to care about the\n",
    "number of bins. Just divide the data into 10 or 20 bins. If you do not have a lot of\n",
    "samples, you can use a simple rule like Sturge’s Rule to calculate the appropriate\n",
    "number of bins.\n",
    "\n",
    "\n",
    "Number of Bins = 1 + log2(N) Where N is the number of samples you have in your dataset. # 12 in our case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the stratification we will use the most correlated attribute (alchohol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strater (col_item):\n",
    "    if col_item <= 9.3:\n",
    "        return 1\n",
    "    elif col_item > 9.3 and col_item <= 10:\n",
    "        return 2\n",
    "    elif col_item > 10 and col_item <= 11:\n",
    "        return 3\n",
    "    elif col_item > 11 and col_item <= 12:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enriq\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_model[\"strat\"] = df_model[\"alcohol\"].apply(strater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits = 1, test_size=0.20, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in split.split(df_model, df_model[\"strat\"]):\n",
    "    strat_train_set = df_model.loc[train_index]\n",
    "    strat_test_set = df_model.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model.drop(\"quality\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we go with stratified split then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.drop(\"strat\", axis=1, inplace=True)\n",
    "strat_test_set.drop(\"strat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = strat_train_set.drop(\"quality\", axis=1)\n",
    "y_train_full = strat_train_set[\"quality\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = strat_test_set.drop(\"quality\", axis=1)\n",
    "y_test = strat_test_set[\"quality\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_full[300:]\n",
    "X_valid = X_train_full[:300]\n",
    "y_train = y_train_full[300:]\n",
    "y_valid = y_train_full[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1596, 14)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976, 8)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 8)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 8)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Model\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "keras.layers.Input(shape=(8,)),\n",
    "keras.layers.Dense(300, activation=\"selu\",  kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "keras.layers.Dense(200, activation=\"selu\", kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "keras.layers.Dense(100, activation=\"selu\",  kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "keras.layers.Dense(50, activation=\"selu\",  kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "keras.layers.Dropout(rate=0.2),\n",
    "keras.layers.Dense(6, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) compile\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer= keras.optimizers.SGD(lr=0.005),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) track\n",
    "# 1) Let’s start by defining the root log directory we will use for our TensorBoard logs\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "# 2) plus a small function that generates a subdirectory path for the current datetime so that it’s different at every \n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_SGD%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def exponential_decay(lr0, s): # with this piece of code and using decay in the optimizer we are able to use exponential deacay\n",
    "#     # we start with high learning rate (0.01) and decrease  by 10 e very s steps. We can see how this helped reach better accuracy in less time\n",
    "#     def exponential_decay_fn(epoch):\n",
    "#         return lr0 * 0.1**(epoch / s)\n",
    "#     return exponential_decay_fn\n",
    "\n",
    "# exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 11384), started 0:02:28 ago. (Use '!kill 11384' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fc8b051f3d181429\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fc8b051f3d181429\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 2/31 [>.............................] - ETA: 9s - loss: 6.5825 - accuracy: 0.2344WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.6709s). Check your callbacks.\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 5.9283 - accuracy: 0.4621 - val_loss: 5.6149 - val_accuracy: 0.5900\n",
      "Epoch 2/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.5879 - accuracy: 0.5748 - val_loss: 5.5296 - val_accuracy: 0.5633\n",
      "Epoch 3/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.5044 - accuracy: 0.5758 - val_loss: 5.4998 - val_accuracy: 0.5833\n",
      "Epoch 4/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.4626 - accuracy: 0.5727 - val_loss: 5.4510 - val_accuracy: 0.5767\n",
      "Epoch 5/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 5.4156 - accuracy: 0.5840 - val_loss: 5.4463 - val_accuracy: 0.5567\n",
      "Epoch 6/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.3813 - accuracy: 0.5902 - val_loss: 5.3991 - val_accuracy: 0.5733\n",
      "Epoch 7/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.3483 - accuracy: 0.5912 - val_loss: 5.3891 - val_accuracy: 0.5533\n",
      "Epoch 8/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.3117 - accuracy: 0.5850 - val_loss: 5.3406 - val_accuracy: 0.5733\n",
      "Epoch 9/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.3010 - accuracy: 0.6045 - val_loss: 5.3185 - val_accuracy: 0.5600\n",
      "Epoch 10/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 5.2634 - accuracy: 0.5768 - val_loss: 5.2988 - val_accuracy: 0.5900\n",
      "Epoch 11/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.2245 - accuracy: 0.5912 - val_loss: 5.2561 - val_accuracy: 0.5700\n",
      "Epoch 12/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.1744 - accuracy: 0.6035 - val_loss: 5.2369 - val_accuracy: 0.5700\n",
      "Epoch 13/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.1586 - accuracy: 0.6076 - val_loss: 5.2261 - val_accuracy: 0.5467\n",
      "Epoch 14/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.1312 - accuracy: 0.5994 - val_loss: 5.1826 - val_accuracy: 0.5733\n",
      "Epoch 15/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.0965 - accuracy: 0.5973 - val_loss: 5.1561 - val_accuracy: 0.5767\n",
      "Epoch 16/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 5.0602 - accuracy: 0.6107 - val_loss: 5.1309 - val_accuracy: 0.6067\n",
      "Epoch 17/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 5.0352 - accuracy: 0.6127 - val_loss: 5.1179 - val_accuracy: 0.5667\n",
      "Epoch 18/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 5.0293 - accuracy: 0.5881 - val_loss: 5.0826 - val_accuracy: 0.5700\n",
      "Epoch 19/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.9782 - accuracy: 0.6363 - val_loss: 5.0572 - val_accuracy: 0.5767\n",
      "Epoch 20/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 4.9561 - accuracy: 0.6066 - val_loss: 5.0366 - val_accuracy: 0.5900\n",
      "Epoch 21/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 4.9188 - accuracy: 0.5963 - val_loss: 5.0061 - val_accuracy: 0.5967\n",
      "Epoch 22/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.9042 - accuracy: 0.6035 - val_loss: 4.9837 - val_accuracy: 0.5767\n",
      "Epoch 23/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.8782 - accuracy: 0.6107 - val_loss: 4.9548 - val_accuracy: 0.5833\n",
      "Epoch 24/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.8582 - accuracy: 0.6096 - val_loss: 4.9316 - val_accuracy: 0.5733\n",
      "Epoch 25/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.8307 - accuracy: 0.6270 - val_loss: 4.9053 - val_accuracy: 0.5767\n",
      "Epoch 26/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.8064 - accuracy: 0.6127 - val_loss: 4.8909 - val_accuracy: 0.5833\n",
      "Epoch 27/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.7789 - accuracy: 0.6189 - val_loss: 4.8606 - val_accuracy: 0.5900\n",
      "Epoch 28/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.7456 - accuracy: 0.6127 - val_loss: 4.8298 - val_accuracy: 0.5833\n",
      "Epoch 29/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 4.7272 - accuracy: 0.6148 - val_loss: 4.8090 - val_accuracy: 0.5833\n",
      "Epoch 30/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 4.7103 - accuracy: 0.6178 - val_loss: 4.7897 - val_accuracy: 0.5800\n",
      "Epoch 31/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 4.6704 - accuracy: 0.6291 - val_loss: 4.7719 - val_accuracy: 0.5633\n",
      "Epoch 32/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.6631 - accuracy: 0.6107 - val_loss: 4.7463 - val_accuracy: 0.6000\n",
      "Epoch 33/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.6322 - accuracy: 0.6219 - val_loss: 4.7197 - val_accuracy: 0.5933\n",
      "Epoch 34/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.6119 - accuracy: 0.6086 - val_loss: 4.6970 - val_accuracy: 0.5933\n",
      "Epoch 35/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.5865 - accuracy: 0.6004 - val_loss: 4.6750 - val_accuracy: 0.5900\n",
      "Epoch 36/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.5611 - accuracy: 0.6270 - val_loss: 4.6520 - val_accuracy: 0.5800\n",
      "Epoch 37/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 4.5374 - accuracy: 0.6209 - val_loss: 4.6302 - val_accuracy: 0.6033\n",
      "Epoch 38/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 4.5101 - accuracy: 0.6148 - val_loss: 4.6061 - val_accuracy: 0.5800\n",
      "Epoch 39/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.4865 - accuracy: 0.6281 - val_loss: 4.5846 - val_accuracy: 0.5933\n",
      "Epoch 40/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.4762 - accuracy: 0.6076 - val_loss: 4.5647 - val_accuracy: 0.5933\n",
      "Epoch 41/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 4.4396 - accuracy: 0.6414 - val_loss: 4.5423 - val_accuracy: 0.5867\n",
      "Epoch 42/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.4303 - accuracy: 0.6209 - val_loss: 4.5236 - val_accuracy: 0.5833\n",
      "Epoch 43/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 4.3978 - accuracy: 0.6199 - val_loss: 4.4985 - val_accuracy: 0.5867\n",
      "Epoch 44/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 4.3841 - accuracy: 0.6281 - val_loss: 4.4797 - val_accuracy: 0.5933\n",
      "Epoch 45/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 4.3661 - accuracy: 0.6352 - val_loss: 4.4598 - val_accuracy: 0.5867\n",
      "Epoch 46/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.3361 - accuracy: 0.6352 - val_loss: 4.4381 - val_accuracy: 0.5867\n",
      "Epoch 47/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.3172 - accuracy: 0.6322 - val_loss: 4.4113 - val_accuracy: 0.5833\n",
      "Epoch 48/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.3000 - accuracy: 0.6178 - val_loss: 4.3921 - val_accuracy: 0.5967\n",
      "Epoch 49/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.2639 - accuracy: 0.6230 - val_loss: 4.3768 - val_accuracy: 0.5733\n",
      "Epoch 50/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.2395 - accuracy: 0.6424 - val_loss: 4.3502 - val_accuracy: 0.5900\n",
      "Epoch 51/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.2339 - accuracy: 0.6250 - val_loss: 4.3323 - val_accuracy: 0.5733\n",
      "Epoch 52/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.2074 - accuracy: 0.6219 - val_loss: 4.3082 - val_accuracy: 0.5867\n",
      "Epoch 53/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.1723 - accuracy: 0.6383 - val_loss: 4.2930 - val_accuracy: 0.5900\n",
      "Epoch 54/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.1714 - accuracy: 0.6332 - val_loss: 4.2816 - val_accuracy: 0.5733\n",
      "Epoch 55/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.1361 - accuracy: 0.6393 - val_loss: 4.2582 - val_accuracy: 0.6000\n",
      "Epoch 56/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.1230 - accuracy: 0.6281 - val_loss: 4.2299 - val_accuracy: 0.5867\n",
      "Epoch 57/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.1151 - accuracy: 0.6373 - val_loss: 4.2118 - val_accuracy: 0.6100\n",
      "Epoch 58/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.0894 - accuracy: 0.6270 - val_loss: 4.1914 - val_accuracy: 0.5933\n",
      "Epoch 59/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.0714 - accuracy: 0.6311 - val_loss: 4.1793 - val_accuracy: 0.5933\n",
      "Epoch 60/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 4.0559 - accuracy: 0.6373 - val_loss: 4.1548 - val_accuracy: 0.5967\n",
      "Epoch 61/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.0320 - accuracy: 0.6291 - val_loss: 4.1383 - val_accuracy: 0.5900\n",
      "Epoch 62/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.0069 - accuracy: 0.6434 - val_loss: 4.1242 - val_accuracy: 0.5833\n",
      "Epoch 63/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 4.0043 - accuracy: 0.6178 - val_loss: 4.1079 - val_accuracy: 0.5733\n",
      "Epoch 64/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 3.9571 - accuracy: 0.6445 - val_loss: 4.0846 - val_accuracy: 0.5733\n",
      "Epoch 65/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.9539 - accuracy: 0.6291 - val_loss: 4.0664 - val_accuracy: 0.5833\n",
      "Epoch 66/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.9318 - accuracy: 0.6486 - val_loss: 4.0620 - val_accuracy: 0.5567\n",
      "Epoch 67/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.9108 - accuracy: 0.6270 - val_loss: 4.0252 - val_accuracy: 0.5900\n",
      "Epoch 68/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.8977 - accuracy: 0.6342 - val_loss: 4.0185 - val_accuracy: 0.5933\n",
      "Epoch 69/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.8831 - accuracy: 0.6363 - val_loss: 3.9897 - val_accuracy: 0.6033\n",
      "Epoch 70/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.8623 - accuracy: 0.6250 - val_loss: 3.9768 - val_accuracy: 0.5767\n",
      "Epoch 71/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.8263 - accuracy: 0.6363 - val_loss: 3.9618 - val_accuracy: 0.5767\n",
      "Epoch 72/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.8162 - accuracy: 0.6547 - val_loss: 3.9373 - val_accuracy: 0.6100\n",
      "Epoch 73/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.8008 - accuracy: 0.6342 - val_loss: 3.9263 - val_accuracy: 0.6067\n",
      "Epoch 74/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.7843 - accuracy: 0.6414 - val_loss: 3.9043 - val_accuracy: 0.5900\n",
      "Epoch 75/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.7632 - accuracy: 0.6568 - val_loss: 3.8890 - val_accuracy: 0.5900\n",
      "Epoch 76/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.7528 - accuracy: 0.6373 - val_loss: 3.8769 - val_accuracy: 0.5733\n",
      "Epoch 77/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.7229 - accuracy: 0.6445 - val_loss: 3.8531 - val_accuracy: 0.5900\n",
      "Epoch 78/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.7142 - accuracy: 0.6424 - val_loss: 3.8500 - val_accuracy: 0.5667\n",
      "Epoch 79/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.6990 - accuracy: 0.6475 - val_loss: 3.8190 - val_accuracy: 0.5767\n",
      "Epoch 80/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.6701 - accuracy: 0.6486 - val_loss: 3.7960 - val_accuracy: 0.5933\n",
      "Epoch 81/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.6488 - accuracy: 0.6527 - val_loss: 3.7857 - val_accuracy: 0.5833\n",
      "Epoch 82/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.6373 - accuracy: 0.6475 - val_loss: 3.7698 - val_accuracy: 0.5867\n",
      "Epoch 83/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.6329 - accuracy: 0.6260 - val_loss: 3.7499 - val_accuracy: 0.5900\n",
      "Epoch 84/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.6065 - accuracy: 0.6373 - val_loss: 3.7363 - val_accuracy: 0.5767\n",
      "Epoch 85/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.5903 - accuracy: 0.6475 - val_loss: 3.7240 - val_accuracy: 0.5900\n",
      "Epoch 86/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.5793 - accuracy: 0.6363 - val_loss: 3.7186 - val_accuracy: 0.5733\n",
      "Epoch 87/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.5641 - accuracy: 0.6373 - val_loss: 3.6971 - val_accuracy: 0.5800\n",
      "Epoch 88/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.5450 - accuracy: 0.6322 - val_loss: 3.6828 - val_accuracy: 0.5800\n",
      "Epoch 89/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.5429 - accuracy: 0.6332 - val_loss: 3.6562 - val_accuracy: 0.5900\n",
      "Epoch 90/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.5132 - accuracy: 0.6465 - val_loss: 3.6413 - val_accuracy: 0.5967\n",
      "Epoch 91/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.4890 - accuracy: 0.6516 - val_loss: 3.6245 - val_accuracy: 0.5867\n",
      "Epoch 92/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.4833 - accuracy: 0.6414 - val_loss: 3.6074 - val_accuracy: 0.5733\n",
      "Epoch 93/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.4634 - accuracy: 0.6352 - val_loss: 3.5911 - val_accuracy: 0.6000\n",
      "Epoch 94/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.4517 - accuracy: 0.6311 - val_loss: 3.5747 - val_accuracy: 0.5900\n",
      "Epoch 95/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.4367 - accuracy: 0.6404 - val_loss: 3.5542 - val_accuracy: 0.6033\n",
      "Epoch 96/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.4193 - accuracy: 0.6486 - val_loss: 3.5408 - val_accuracy: 0.5900\n",
      "Epoch 97/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.3916 - accuracy: 0.6291 - val_loss: 3.5367 - val_accuracy: 0.5800\n",
      "Epoch 98/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.3916 - accuracy: 0.6455 - val_loss: 3.5107 - val_accuracy: 0.5833\n",
      "Epoch 99/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.3675 - accuracy: 0.6465 - val_loss: 3.5065 - val_accuracy: 0.5933\n",
      "Epoch 100/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.3563 - accuracy: 0.6445 - val_loss: 3.4787 - val_accuracy: 0.5900\n",
      "Epoch 101/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.3476 - accuracy: 0.6445 - val_loss: 3.4685 - val_accuracy: 0.5733\n",
      "Epoch 102/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.3188 - accuracy: 0.6465 - val_loss: 3.4525 - val_accuracy: 0.5767\n",
      "Epoch 103/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.3159 - accuracy: 0.6455 - val_loss: 3.4348 - val_accuracy: 0.5967\n",
      "Epoch 104/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.2802 - accuracy: 0.6537 - val_loss: 3.4340 - val_accuracy: 0.5867\n",
      "Epoch 105/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.2771 - accuracy: 0.6486 - val_loss: 3.4114 - val_accuracy: 0.6067\n",
      "Epoch 106/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.2647 - accuracy: 0.6455 - val_loss: 3.3974 - val_accuracy: 0.5900\n",
      "Epoch 107/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.2488 - accuracy: 0.6434 - val_loss: 3.3858 - val_accuracy: 0.5900\n",
      "Epoch 108/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.2383 - accuracy: 0.6373 - val_loss: 3.3669 - val_accuracy: 0.5967\n",
      "Epoch 109/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.2173 - accuracy: 0.6527 - val_loss: 3.3517 - val_accuracy: 0.5900\n",
      "Epoch 110/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.2064 - accuracy: 0.6404 - val_loss: 3.3439 - val_accuracy: 0.5967\n",
      "Epoch 111/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.1877 - accuracy: 0.6516 - val_loss: 3.3308 - val_accuracy: 0.5800\n",
      "Epoch 112/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.1825 - accuracy: 0.6404 - val_loss: 3.3112 - val_accuracy: 0.5900\n",
      "Epoch 113/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 3.1588 - accuracy: 0.6527 - val_loss: 3.3192 - val_accuracy: 0.5733\n",
      "Epoch 114/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.1484 - accuracy: 0.6404 - val_loss: 3.2998 - val_accuracy: 0.6000\n",
      "Epoch 115/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.1411 - accuracy: 0.6516 - val_loss: 3.2728 - val_accuracy: 0.5833\n",
      "Epoch 116/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.1356 - accuracy: 0.6342 - val_loss: 3.2633 - val_accuracy: 0.5900\n",
      "Epoch 117/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 3.1148 - accuracy: 0.6506 - val_loss: 3.2598 - val_accuracy: 0.5833\n",
      "Epoch 118/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.1005 - accuracy: 0.6465 - val_loss: 3.2372 - val_accuracy: 0.5933\n",
      "Epoch 119/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 3.0848 - accuracy: 0.6557 - val_loss: 3.2425 - val_accuracy: 0.5733\n",
      "Epoch 120/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.0721 - accuracy: 0.6506 - val_loss: 3.2219 - val_accuracy: 0.5900\n",
      "Epoch 121/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.0651 - accuracy: 0.6465 - val_loss: 3.1956 - val_accuracy: 0.5933\n",
      "Epoch 122/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.0378 - accuracy: 0.6434 - val_loss: 3.1945 - val_accuracy: 0.5900\n",
      "Epoch 123/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.0305 - accuracy: 0.6465 - val_loss: 3.1699 - val_accuracy: 0.5900\n",
      "Epoch 124/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 3.0134 - accuracy: 0.6414 - val_loss: 3.1700 - val_accuracy: 0.5767\n",
      "Epoch 125/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 3.0008 - accuracy: 0.6475 - val_loss: 3.1404 - val_accuracy: 0.5733\n",
      "Epoch 126/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.9965 - accuracy: 0.6496 - val_loss: 3.1369 - val_accuracy: 0.6033\n",
      "Epoch 127/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.9908 - accuracy: 0.6537 - val_loss: 3.1335 - val_accuracy: 0.5767\n",
      "Epoch 128/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.9609 - accuracy: 0.6475 - val_loss: 3.1146 - val_accuracy: 0.5733\n",
      "Epoch 129/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.9621 - accuracy: 0.6475 - val_loss: 3.0875 - val_accuracy: 0.6033\n",
      "Epoch 130/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.9520 - accuracy: 0.6332 - val_loss: 3.0819 - val_accuracy: 0.5800\n",
      "Epoch 131/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.9261 - accuracy: 0.6516 - val_loss: 3.0649 - val_accuracy: 0.6167\n",
      "Epoch 132/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.9171 - accuracy: 0.6568 - val_loss: 3.0744 - val_accuracy: 0.5767\n",
      "Epoch 133/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.9069 - accuracy: 0.6547 - val_loss: 3.0619 - val_accuracy: 0.5867\n",
      "Epoch 134/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.9014 - accuracy: 0.6516 - val_loss: 3.0301 - val_accuracy: 0.5800\n",
      "Epoch 135/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.8802 - accuracy: 0.6598 - val_loss: 3.0263 - val_accuracy: 0.6000\n",
      "Epoch 136/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.8636 - accuracy: 0.6393 - val_loss: 3.0223 - val_accuracy: 0.5933\n",
      "Epoch 137/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.8580 - accuracy: 0.6424 - val_loss: 3.0260 - val_accuracy: 0.5733\n",
      "Epoch 138/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.8456 - accuracy: 0.6527 - val_loss: 2.9804 - val_accuracy: 0.6033\n",
      "Epoch 139/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.8380 - accuracy: 0.6537 - val_loss: 2.9728 - val_accuracy: 0.5967\n",
      "Epoch 140/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.8243 - accuracy: 0.6578 - val_loss: 2.9669 - val_accuracy: 0.5833\n",
      "Epoch 141/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.8120 - accuracy: 0.6496 - val_loss: 2.9515 - val_accuracy: 0.5833\n",
      "Epoch 142/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.8054 - accuracy: 0.6568 - val_loss: 2.9482 - val_accuracy: 0.5800\n",
      "Epoch 143/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.7866 - accuracy: 0.6455 - val_loss: 2.9304 - val_accuracy: 0.5967\n",
      "Epoch 144/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.7753 - accuracy: 0.6434 - val_loss: 2.9319 - val_accuracy: 0.5800\n",
      "Epoch 145/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.7732 - accuracy: 0.6414 - val_loss: 2.9114 - val_accuracy: 0.6033\n",
      "Epoch 146/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.7518 - accuracy: 0.6527 - val_loss: 2.8918 - val_accuracy: 0.5900\n",
      "Epoch 147/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.7385 - accuracy: 0.6527 - val_loss: 2.8838 - val_accuracy: 0.5867\n",
      "Epoch 148/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.7309 - accuracy: 0.6414 - val_loss: 2.8831 - val_accuracy: 0.5800\n",
      "Epoch 149/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.7163 - accuracy: 0.6609 - val_loss: 2.8713 - val_accuracy: 0.6000\n",
      "Epoch 150/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.7097 - accuracy: 0.6527 - val_loss: 2.8510 - val_accuracy: 0.5967\n",
      "Epoch 151/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.7022 - accuracy: 0.6465 - val_loss: 2.8520 - val_accuracy: 0.5700\n",
      "Epoch 152/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.6906 - accuracy: 0.6496 - val_loss: 2.8513 - val_accuracy: 0.5700\n",
      "Epoch 153/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.6757 - accuracy: 0.6506 - val_loss: 2.8155 - val_accuracy: 0.5900\n",
      "Epoch 154/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.6577 - accuracy: 0.6516 - val_loss: 2.8203 - val_accuracy: 0.5700\n",
      "Epoch 155/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.6496 - accuracy: 0.6557 - val_loss: 2.8048 - val_accuracy: 0.6100\n",
      "Epoch 156/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.6480 - accuracy: 0.6445 - val_loss: 2.8032 - val_accuracy: 0.6000\n",
      "Epoch 157/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.6434 - accuracy: 0.6434 - val_loss: 2.7769 - val_accuracy: 0.5933\n",
      "Epoch 158/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.6258 - accuracy: 0.6568 - val_loss: 2.7655 - val_accuracy: 0.6067\n",
      "Epoch 159/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.6105 - accuracy: 0.6557 - val_loss: 2.7605 - val_accuracy: 0.6033\n",
      "Epoch 160/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.5992 - accuracy: 0.6609 - val_loss: 2.7623 - val_accuracy: 0.6000\n",
      "Epoch 161/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.5994 - accuracy: 0.6516 - val_loss: 2.7381 - val_accuracy: 0.6067\n",
      "Epoch 162/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.5882 - accuracy: 0.6516 - val_loss: 2.7354 - val_accuracy: 0.5933\n",
      "Epoch 163/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.5643 - accuracy: 0.6588 - val_loss: 2.7197 - val_accuracy: 0.5833\n",
      "Epoch 164/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.5557 - accuracy: 0.6527 - val_loss: 2.7100 - val_accuracy: 0.6067\n",
      "Epoch 165/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.5544 - accuracy: 0.6496 - val_loss: 2.7142 - val_accuracy: 0.6167\n",
      "Epoch 166/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.5468 - accuracy: 0.6496 - val_loss: 2.6961 - val_accuracy: 0.5900\n",
      "Epoch 167/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.5249 - accuracy: 0.6455 - val_loss: 2.6986 - val_accuracy: 0.5800\n",
      "Epoch 168/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.5295 - accuracy: 0.6486 - val_loss: 2.6693 - val_accuracy: 0.5900\n",
      "Epoch 169/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.5096 - accuracy: 0.6670 - val_loss: 2.6843 - val_accuracy: 0.5833\n",
      "Epoch 170/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.5059 - accuracy: 0.6455 - val_loss: 2.6619 - val_accuracy: 0.5800\n",
      "Epoch 171/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.4915 - accuracy: 0.6434 - val_loss: 2.6369 - val_accuracy: 0.5800\n",
      "Epoch 172/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.4779 - accuracy: 0.6506 - val_loss: 2.6416 - val_accuracy: 0.5900\n",
      "Epoch 173/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.4719 - accuracy: 0.6516 - val_loss: 2.6195 - val_accuracy: 0.5900\n",
      "Epoch 174/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.4661 - accuracy: 0.6568 - val_loss: 2.6259 - val_accuracy: 0.5633\n",
      "Epoch 175/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.4567 - accuracy: 0.6383 - val_loss: 2.6055 - val_accuracy: 0.6000\n",
      "Epoch 176/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.4449 - accuracy: 0.6609 - val_loss: 2.5988 - val_accuracy: 0.5833\n",
      "Epoch 177/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.4343 - accuracy: 0.6598 - val_loss: 2.5920 - val_accuracy: 0.6000\n",
      "Epoch 178/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.4251 - accuracy: 0.6475 - val_loss: 2.5735 - val_accuracy: 0.5867\n",
      "Epoch 179/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.4239 - accuracy: 0.6609 - val_loss: 2.5614 - val_accuracy: 0.6033\n",
      "Epoch 180/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.4160 - accuracy: 0.6486 - val_loss: 2.5874 - val_accuracy: 0.5933\n",
      "Epoch 181/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.4049 - accuracy: 0.6455 - val_loss: 2.5556 - val_accuracy: 0.5800\n",
      "Epoch 182/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3882 - accuracy: 0.6537 - val_loss: 2.5479 - val_accuracy: 0.5800\n",
      "Epoch 183/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3871 - accuracy: 0.6557 - val_loss: 2.5517 - val_accuracy: 0.5867\n",
      "Epoch 184/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3825 - accuracy: 0.6568 - val_loss: 2.5195 - val_accuracy: 0.6000\n",
      "Epoch 185/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3608 - accuracy: 0.6547 - val_loss: 2.5332 - val_accuracy: 0.5833\n",
      "Epoch 186/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3510 - accuracy: 0.6598 - val_loss: 2.5109 - val_accuracy: 0.5867\n",
      "Epoch 187/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3505 - accuracy: 0.6609 - val_loss: 2.5025 - val_accuracy: 0.5833\n",
      "Epoch 188/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.3397 - accuracy: 0.6578 - val_loss: 2.4952 - val_accuracy: 0.6167\n",
      "Epoch 189/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3310 - accuracy: 0.6465 - val_loss: 2.4796 - val_accuracy: 0.5900\n",
      "Epoch 190/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3282 - accuracy: 0.6588 - val_loss: 2.4715 - val_accuracy: 0.5867\n",
      "Epoch 191/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3081 - accuracy: 0.6629 - val_loss: 2.4694 - val_accuracy: 0.5900\n",
      "Epoch 192/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3002 - accuracy: 0.6506 - val_loss: 2.4704 - val_accuracy: 0.5733\n",
      "Epoch 193/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3035 - accuracy: 0.6516 - val_loss: 2.4540 - val_accuracy: 0.6033\n",
      "Epoch 194/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.2952 - accuracy: 0.6557 - val_loss: 2.4422 - val_accuracy: 0.6000\n",
      "Epoch 195/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.2869 - accuracy: 0.6588 - val_loss: 2.4325 - val_accuracy: 0.5933\n",
      "Epoch 196/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.2737 - accuracy: 0.6588 - val_loss: 2.4260 - val_accuracy: 0.6033\n",
      "Epoch 197/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.2653 - accuracy: 0.6568 - val_loss: 2.4219 - val_accuracy: 0.5900\n",
      "Epoch 198/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.2555 - accuracy: 0.6486 - val_loss: 2.4124 - val_accuracy: 0.5733\n",
      "Epoch 199/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.2575 - accuracy: 0.6475 - val_loss: 2.4100 - val_accuracy: 0.5967\n",
      "Epoch 200/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.2421 - accuracy: 0.6506 - val_loss: 2.4033 - val_accuracy: 0.6000\n",
      "Epoch 201/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.2356 - accuracy: 0.6557 - val_loss: 2.3844 - val_accuracy: 0.5767\n",
      "Epoch 202/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.2256 - accuracy: 0.6598 - val_loss: 2.3765 - val_accuracy: 0.6000\n",
      "Epoch 203/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.2167 - accuracy: 0.6609 - val_loss: 2.3644 - val_accuracy: 0.6033\n",
      "Epoch 204/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.2059 - accuracy: 0.6547 - val_loss: 2.3674 - val_accuracy: 0.5933\n",
      "Epoch 205/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.2041 - accuracy: 0.6465 - val_loss: 2.3538 - val_accuracy: 0.5967\n",
      "Epoch 206/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.1844 - accuracy: 0.6588 - val_loss: 2.3806 - val_accuracy: 0.5767\n",
      "Epoch 207/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.1827 - accuracy: 0.6588 - val_loss: 2.3499 - val_accuracy: 0.5900\n",
      "Epoch 208/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.1816 - accuracy: 0.6455 - val_loss: 2.3296 - val_accuracy: 0.5900\n",
      "Epoch 209/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.1592 - accuracy: 0.6639 - val_loss: 2.3272 - val_accuracy: 0.5933\n",
      "Epoch 210/300\n",
      "31/31 [==============================] - 0s 6ms/step - loss: 2.1559 - accuracy: 0.6650 - val_loss: 2.3264 - val_accuracy: 0.5767\n",
      "Epoch 211/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.1532 - accuracy: 0.6506 - val_loss: 2.3169 - val_accuracy: 0.5800\n",
      "Epoch 212/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.1420 - accuracy: 0.6639 - val_loss: 2.3178 - val_accuracy: 0.5933\n",
      "Epoch 213/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.1438 - accuracy: 0.6568 - val_loss: 2.3060 - val_accuracy: 0.6033\n",
      "Epoch 214/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.1312 - accuracy: 0.6639 - val_loss: 2.3221 - val_accuracy: 0.5600\n",
      "Epoch 215/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.1185 - accuracy: 0.6578 - val_loss: 2.3033 - val_accuracy: 0.5967\n",
      "Epoch 216/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.1162 - accuracy: 0.6588 - val_loss: 2.3046 - val_accuracy: 0.5833\n",
      "Epoch 217/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.1137 - accuracy: 0.6486 - val_loss: 2.2755 - val_accuracy: 0.5800\n",
      "Epoch 218/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.1044 - accuracy: 0.6516 - val_loss: 2.2638 - val_accuracy: 0.5933\n",
      "Epoch 219/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.1069 - accuracy: 0.6506 - val_loss: 2.2571 - val_accuracy: 0.6033\n",
      "Epoch 220/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.0900 - accuracy: 0.6660 - val_loss: 2.2518 - val_accuracy: 0.5967\n",
      "Epoch 221/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.0857 - accuracy: 0.6568 - val_loss: 2.2378 - val_accuracy: 0.6033\n",
      "Epoch 222/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.0721 - accuracy: 0.6557 - val_loss: 2.2579 - val_accuracy: 0.5900\n",
      "Epoch 223/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.0748 - accuracy: 0.6527 - val_loss: 2.2291 - val_accuracy: 0.5933\n",
      "Epoch 224/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.0632 - accuracy: 0.6639 - val_loss: 2.2501 - val_accuracy: 0.5800\n",
      "Epoch 225/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.0594 - accuracy: 0.6588 - val_loss: 2.2255 - val_accuracy: 0.5967\n",
      "Epoch 226/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.0542 - accuracy: 0.6732 - val_loss: 2.2159 - val_accuracy: 0.6000\n",
      "Epoch 227/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.0382 - accuracy: 0.6701 - val_loss: 2.2370 - val_accuracy: 0.5667\n",
      "Epoch 228/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.0333 - accuracy: 0.6598 - val_loss: 2.2089 - val_accuracy: 0.6100\n",
      "Epoch 229/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.0252 - accuracy: 0.6639 - val_loss: 2.1891 - val_accuracy: 0.5933\n",
      "Epoch 230/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 2.0207 - accuracy: 0.6516 - val_loss: 2.1938 - val_accuracy: 0.6000\n",
      "Epoch 231/300\n",
      "31/31 [==============================] - 0s 6ms/step - loss: 2.0115 - accuracy: 0.6588 - val_loss: 2.1698 - val_accuracy: 0.5900\n",
      "Epoch 232/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.0119 - accuracy: 0.6537 - val_loss: 2.1833 - val_accuracy: 0.5900\n",
      "Epoch 233/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.0031 - accuracy: 0.6475 - val_loss: 2.1598 - val_accuracy: 0.5933\n",
      "Epoch 234/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.9974 - accuracy: 0.6557 - val_loss: 2.2003 - val_accuracy: 0.5600\n",
      "Epoch 235/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.9922 - accuracy: 0.6547 - val_loss: 2.1452 - val_accuracy: 0.6000\n",
      "Epoch 236/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.9740 - accuracy: 0.6609 - val_loss: 2.1453 - val_accuracy: 0.5967\n",
      "Epoch 237/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.9725 - accuracy: 0.6619 - val_loss: 2.1429 - val_accuracy: 0.5833\n",
      "Epoch 238/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.9628 - accuracy: 0.6660 - val_loss: 2.1568 - val_accuracy: 0.5933\n",
      "Epoch 239/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.9618 - accuracy: 0.6578 - val_loss: 2.1299 - val_accuracy: 0.5967\n",
      "Epoch 240/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.9613 - accuracy: 0.6527 - val_loss: 2.1253 - val_accuracy: 0.6133\n",
      "Epoch 241/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.9455 - accuracy: 0.6732 - val_loss: 2.1412 - val_accuracy: 0.5833\n",
      "Epoch 242/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.9360 - accuracy: 0.6680 - val_loss: 2.1302 - val_accuracy: 0.5900\n",
      "Epoch 243/300\n",
      "31/31 [==============================] - 0s 6ms/step - loss: 1.9407 - accuracy: 0.6619 - val_loss: 2.1073 - val_accuracy: 0.5833\n",
      "Epoch 244/300\n",
      "31/31 [==============================] - 0s 6ms/step - loss: 1.9350 - accuracy: 0.6721 - val_loss: 2.0994 - val_accuracy: 0.5900\n",
      "Epoch 245/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.9257 - accuracy: 0.6547 - val_loss: 2.0948 - val_accuracy: 0.6033\n",
      "Epoch 246/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.9322 - accuracy: 0.6578 - val_loss: 2.0877 - val_accuracy: 0.5967\n",
      "Epoch 247/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.9106 - accuracy: 0.6588 - val_loss: 2.1011 - val_accuracy: 0.5967\n",
      "Epoch 248/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.9027 - accuracy: 0.6660 - val_loss: 2.0784 - val_accuracy: 0.5867\n",
      "Epoch 249/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.9062 - accuracy: 0.6537 - val_loss: 2.0783 - val_accuracy: 0.5967\n",
      "Epoch 250/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.8947 - accuracy: 0.6711 - val_loss: 2.0698 - val_accuracy: 0.5933\n",
      "Epoch 251/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.8881 - accuracy: 0.6639 - val_loss: 2.0612 - val_accuracy: 0.6100\n",
      "Epoch 252/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.8780 - accuracy: 0.6568 - val_loss: 2.0509 - val_accuracy: 0.5833\n",
      "Epoch 253/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.8740 - accuracy: 0.6547 - val_loss: 2.0628 - val_accuracy: 0.5867\n",
      "Epoch 254/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.8723 - accuracy: 0.6568 - val_loss: 2.0609 - val_accuracy: 0.5800\n",
      "Epoch 255/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.8668 - accuracy: 0.6670 - val_loss: 2.0491 - val_accuracy: 0.5967\n",
      "Epoch 256/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.8633 - accuracy: 0.6732 - val_loss: 2.0277 - val_accuracy: 0.6033\n",
      "Epoch 257/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.8605 - accuracy: 0.6434 - val_loss: 2.0226 - val_accuracy: 0.5967\n",
      "Epoch 258/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.8399 - accuracy: 0.6660 - val_loss: 2.0342 - val_accuracy: 0.5833\n",
      "Epoch 259/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.8307 - accuracy: 0.6783 - val_loss: 2.0667 - val_accuracy: 0.6000\n",
      "Epoch 260/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.8473 - accuracy: 0.6434 - val_loss: 2.0256 - val_accuracy: 0.5700\n",
      "Epoch 261/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.8311 - accuracy: 0.6670 - val_loss: 2.0296 - val_accuracy: 0.5900\n",
      "Epoch 262/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.8334 - accuracy: 0.6598 - val_loss: 1.9918 - val_accuracy: 0.6000\n",
      "Epoch 263/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.8264 - accuracy: 0.6537 - val_loss: 1.9925 - val_accuracy: 0.5867\n",
      "Epoch 264/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.8130 - accuracy: 0.6619 - val_loss: 1.9901 - val_accuracy: 0.5867\n",
      "Epoch 265/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.8133 - accuracy: 0.6557 - val_loss: 1.9824 - val_accuracy: 0.6067\n",
      "Epoch 266/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7977 - accuracy: 0.6660 - val_loss: 1.9738 - val_accuracy: 0.5800\n",
      "Epoch 267/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7988 - accuracy: 0.6701 - val_loss: 1.9819 - val_accuracy: 0.5800\n",
      "Epoch 268/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.7938 - accuracy: 0.6660 - val_loss: 1.9816 - val_accuracy: 0.5900\n",
      "Epoch 269/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7899 - accuracy: 0.6670 - val_loss: 1.9791 - val_accuracy: 0.6033\n",
      "Epoch 270/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7849 - accuracy: 0.6496 - val_loss: 1.9643 - val_accuracy: 0.5833\n",
      "Epoch 271/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.7836 - accuracy: 0.6598 - val_loss: 1.9441 - val_accuracy: 0.5967\n",
      "Epoch 272/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.7755 - accuracy: 0.6639 - val_loss: 1.9458 - val_accuracy: 0.6100\n",
      "Epoch 273/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7702 - accuracy: 0.6619 - val_loss: 1.9348 - val_accuracy: 0.6167\n",
      "Epoch 274/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7648 - accuracy: 0.6598 - val_loss: 1.9313 - val_accuracy: 0.5900\n",
      "Epoch 275/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7611 - accuracy: 0.6506 - val_loss: 1.9381 - val_accuracy: 0.6133\n",
      "Epoch 276/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.7507 - accuracy: 0.6691 - val_loss: 1.9458 - val_accuracy: 0.6100\n",
      "Epoch 277/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7561 - accuracy: 0.6465 - val_loss: 1.9192 - val_accuracy: 0.6033\n",
      "Epoch 278/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.7422 - accuracy: 0.6547 - val_loss: 1.9560 - val_accuracy: 0.5700\n",
      "Epoch 279/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.7432 - accuracy: 0.6711 - val_loss: 1.9443 - val_accuracy: 0.5833\n",
      "Epoch 280/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7389 - accuracy: 0.6568 - val_loss: 1.9229 - val_accuracy: 0.6100\n",
      "Epoch 281/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.7369 - accuracy: 0.6506 - val_loss: 1.9037 - val_accuracy: 0.5967\n",
      "Epoch 282/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.7220 - accuracy: 0.6537 - val_loss: 1.9398 - val_accuracy: 0.5633\n",
      "Epoch 283/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7261 - accuracy: 0.6619 - val_loss: 1.9087 - val_accuracy: 0.5800\n",
      "Epoch 284/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.7190 - accuracy: 0.6609 - val_loss: 1.9121 - val_accuracy: 0.5900\n",
      "Epoch 285/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.7100 - accuracy: 0.6691 - val_loss: 1.8929 - val_accuracy: 0.6000\n",
      "Epoch 286/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.7038 - accuracy: 0.6639 - val_loss: 1.8797 - val_accuracy: 0.5967\n",
      "Epoch 287/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.6978 - accuracy: 0.6568 - val_loss: 1.8925 - val_accuracy: 0.5967\n",
      "Epoch 288/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.6943 - accuracy: 0.6598 - val_loss: 1.8944 - val_accuracy: 0.5900\n",
      "Epoch 289/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.6902 - accuracy: 0.6670 - val_loss: 1.8951 - val_accuracy: 0.5733\n",
      "Epoch 290/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.6813 - accuracy: 0.6721 - val_loss: 1.8791 - val_accuracy: 0.5867\n",
      "Epoch 291/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.6861 - accuracy: 0.6711 - val_loss: 1.8491 - val_accuracy: 0.6033\n",
      "Epoch 292/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.6844 - accuracy: 0.6619 - val_loss: 1.8489 - val_accuracy: 0.6033\n",
      "Epoch 293/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.6820 - accuracy: 0.6506 - val_loss: 1.8557 - val_accuracy: 0.6000\n",
      "Epoch 294/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.6739 - accuracy: 0.6680 - val_loss: 1.8460 - val_accuracy: 0.5733\n",
      "Epoch 295/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.6659 - accuracy: 0.6639 - val_loss: 1.8665 - val_accuracy: 0.5700\n",
      "Epoch 296/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.6604 - accuracy: 0.6629 - val_loss: 1.8539 - val_accuracy: 0.5933\n",
      "Epoch 297/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 1.6498 - accuracy: 0.6629 - val_loss: 1.8385 - val_accuracy: 0.5833\n",
      "Epoch 298/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.6432 - accuracy: 0.6609 - val_loss: 1.8287 - val_accuracy: 0.6067\n",
      "Epoch 299/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.6480 - accuracy: 0.6660 - val_loss: 1.8240 - val_accuracy: 0.6000\n",
      "Epoch 300/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 1.6458 - accuracy: 0.6537 - val_loss: 1.8202 - val_accuracy: 0.6133\n"
     ]
    }
   ],
   "source": [
    "# 4) Fit\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_mnist_model.h5\", save_best_only=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "# lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=300,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[early_stopping_cb, checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_wine_class_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_wine_class_model.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 1.7036 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7035995721817017, 0.0]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 8)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = [np.argmax(i) for i in y_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 6), dtype=int32, numpy=\n",
       "array([[  0,   0,   2,   0,   0,   0],\n",
       "       [  0,   0,   5,   1,   0,   0],\n",
       "       [  0,   0, 116,  31,   1,   0],\n",
       "       [  0,   0,  48,  65,  12,   0],\n",
       "       [  0,   0,   2,  17,  16,   0],\n",
       "       [  0,   0,   0,   1,   3,   0]])>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.confusion_matrix(labels=y_test, predictions=y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "y_predicted_labels = [np.argmax(i) for i in y_predicted]\n",
    "corr_x = tf.math.confusion_matrix(labels=y_test,predictions=y_predicted_labels)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(corr_x, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.savefig(\"corr-mat.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
